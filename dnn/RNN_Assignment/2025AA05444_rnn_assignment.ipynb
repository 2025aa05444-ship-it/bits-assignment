{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc017f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "DEEP NEURAL NETWORKS - ASSIGNMENT 3: RNN vs TRANSFORMER FOR TIME SERIES\n",
    "Recurrent Neural Networks vs Transformers for Time Series Prediction\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b67847",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "STUDENT INFORMATION (REQUIRED - DO NOT DELETE)\n",
    "================================================================================\n",
    "\n",
    "BITS ID: 2025AA05444\n",
    "Name: Prasad\n",
    "Email: [Enter your email]\n",
    "Date: 2025-02-07\n",
    "\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "ASSIGNMENT OVERVIEW\n",
    "================================================================================\n",
    "\n",
    "This assignment requires you to implement and compare two approaches for \n",
    "time series forecasting:\n",
    "1. LSTM or GRU using Keras/PyTorch\n",
    "2. Transformer encoder using Keras/PyTorch layers\n",
    "\n",
    "Learning Objectives:\n",
    "- Build recurrent neural networks for sequential data\n",
    "- Use transformer architecture for time series\n",
    "- Implement or integrate positional encoding\n",
    "- Compare RNN vs Transformer architectures\n",
    "- Understand time series preprocessing and evaluation\n",
    "\n",
    "IMPORTANT: \n",
    "- Positional encoding MUST be added to transformer\n",
    "- Use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\n",
    "- DO NOT use pre-trained transformers (HuggingFace, TimeGPT, etc.)\n",
    "- Use temporal train/test split (NO shuffling)\n",
    "\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b05f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "STUDENT INFORMATION (REQUIRED - DO NOT DELETE)\n",
    "================================================================================\n",
    "\n",
    "BITS ID: 2025AA05444\n",
    "Name: Prasad\n",
    "Email: [Enter your email]\n",
    "Date: 2025-02-07\n",
    "\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7883103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b889b4",
   "metadata": {},
   "source": [
    "Deep learning frameworks (choose Keras or PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c89de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "PART 1: DATASET LOADING AND EXPLORATION (Informational)\n",
    "================================================================================\n",
    "\n",
    "Instructions:\n",
    "1. Choose ONE dataset from the allowed list\n",
    "2. Load and explore the time series data\n",
    "3. Fill in ALL required metadata fields below\n",
    "4. Provide justification for your primary metric choice\n",
    "\n",
    "ALLOWED DATASETS:\n",
    "- Stock Prices (daily/hourly closing prices)\n",
    "- Weather Data (temperature, humidity, pressure)\n",
    "- Energy Consumption (electricity/power usage)\n",
    "- Sensor Data (IoT sensor readings)\n",
    "- Custom time series (with approval)\n",
    "\n",
    "REQUIRED OUTPUT:\n",
    "- Print all metadata fields\n",
    "- Time series plots\n",
    "- Stationarity analysis\n",
    "- Train/test split visualization\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c8fef3",
   "metadata": {},
   "source": [
    "1.1 Dataset Selection and Loading\n",
    "TODO: Load your chosen time series dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Dataset Selection and Loading\n",
    "# Loading IBM stock data\n",
    "url = \"https://raw.githubusercontent.com/plotly/datasets/master/stockdata.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df = df[['Date', 'IBM']]\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# Limit to first 1500 records as requested\n",
    "df = df.iloc[:1500]\n",
    "\n",
    "# Use IBM price as the time series\n",
    "data_series = df['IBM'].values.reshape(-1, 1)\n",
    "\n",
    "print(f\"Loaded {len(df)} records for IBM stock.\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b9ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED: Fill in these metadata fields\n",
    "dataset_name = \"IBM Stock Prices\"\n",
    "dataset_source = \"Plotly Datasets (https://raw.githubusercontent.com/plotly/datasets/master/stockdata.csv)\"\n",
    "n_samples = 1500\n",
    "n_features = 1\n",
    "sequence_length = 50\n",
    "prediction_horizon = 5\n",
    "problem_type = \"time_series_forecasting\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae533b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary metric selection\n",
    "primary_metric = \"RMSE\"\n",
    "metric_justification = \"\"\"\n",
    "RMSE (Root Mean Squared Error) is chosen because it penalizes larger errors more heavily, \n",
    "which is crucial in financial forecasting where large deviations can lead to significant losses.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef57a2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Source: {dataset_source}\")\n",
    "print(f\"Total Samples: {n_samples}\")\n",
    "print(f\"Number of Features: {n_features}\")\n",
    "print(f\"Sequence Length: {sequence_length}\")\n",
    "print(f\"Prediction Horizon: {prediction_horizon}\")\n",
    "print(f\"Primary Metric: {primary_metric}\")\n",
    "print(f\"Metric Justification: {metric_justification}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd8ddfa",
   "metadata": {},
   "source": [
    "1.2 Time Series Exploration\n",
    "TODO: Plot time series data\n",
    "TODO: Check for trends, seasonality\n",
    "TODO: Perform stationarity tests (optional but recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Time Series Exploration\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(df['Date'], df['IBM'])\n",
    "plt.title('IBM Stock Price (First 1500 Records)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Stationarity check (Visual)\n",
    "# Rolling mean and std\n",
    "rolling_mean = df['IBM'].rolling(window=30).mean()\n",
    "rolling_std = df['IBM'].rolling(window=30).std()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(df['Date'], df['IBM'], label='Original')\n",
    "plt.plot(df['Date'], rolling_mean, label='Rolling Mean (30d)')\n",
    "plt.plot(df['Date'], rolling_std, label='Rolling Std (30d)')\n",
    "plt.legend()\n",
    "plt.title('Stationarity Check: Rolling Mean & Standard Deviation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d498e253",
   "metadata": {},
   "source": [
    "1.3 Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efdacdc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_timeseries(data):\n",
    "    \"\"\"\n",
    "    Preprocess time series data\n",
    "    \n",
    "    Args:\n",
    "        data: raw time series data\n",
    "    \n",
    "    Returns:\n",
    "        preprocessed data, scaler\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    \n",
    "    return scaled_data, scaler\n",
    "\n",
    "# Preprocess the data\n",
    "scaled_data, scaler = preprocess_timeseries(data_series)\n",
    "print(f\"Data scaled. Shape: {scaled_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0da181",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length, pred_horizon):\n",
    "    \"\"\"\n",
    "    Create sequences for time series prediction\n",
    "    \n",
    "    Args:\n",
    "        data: preprocessed time series data\n",
    "        seq_length: lookback window\n",
    "        pred_horizon: forecast steps ahead\n",
    "    \n",
    "    Returns:\n",
    "        X: input sequences, y: target values\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length - pred_horizon + 1):\n",
    "        X.append(data[i:(i + seq_length)])\n",
    "        y.append(data[(i + seq_length):(i + seq_length + pred_horizon)])\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(scaled_data, sequence_length, prediction_horizon)\n",
    "print(f\"Sequences created. X shape: {X.shape}, y shape: {y.shape}\")\n",
    "y = y.reshape(y.shape[0], y.shape[1])\n",
    "print(f\"Reshaped y: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d888128f",
   "metadata": {},
   "source": [
    "TODO: Preprocess data\n",
    "TODO: Create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED: Temporal train/test split (NO SHUFFLING)\n",
    "train_test_ratio = \"90/10\"\n",
    "split_index = int(len(X) * 0.9)\n",
    "\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "train_samples = len(X_train)\n",
    "test_samples = len(X_test)\n",
    "\n",
    "print(f\"\\nTrain/Test Split: {train_test_ratio}\")\n",
    "print(f\"Training Samples: {train_samples}\")\n",
    "print(f\"Test Samples: {test_samples}\")\n",
    "print(\"\u26a0\ufe0f  IMPORTANT: Temporal split used (NO shuffling)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa1ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTrain/Test Split: {train_test_ratio}\")\n",
    "print(f\"Training Samples: {train_samples}\")\n",
    "print(f\"Test Samples: {test_samples}\")\n",
    "print(\"\u26a0\ufe0f  IMPORTANT: Temporal split used (NO shuffling)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef664ae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "PART 2: LSTM/GRU IMPLEMENTATION (5 MARKS)\n",
    "================================================================================\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Build LSTM OR GRU using Keras/PyTorch layers\n",
    "- Architecture must include:\n",
    "  * At least 2 stacked recurrent layers\n",
    "  * Output layer for prediction\n",
    "- Use model.compile() and model.fit() (Keras) OR standard PyTorch training\n",
    "- Track initial_loss and final_loss\n",
    "\n",
    "GRADING:\n",
    "- LSTM/GRU architecture with stacked layers: 2 marks\n",
    "- Model properly compiled/configured: 1 mark\n",
    "- Training completed with loss tracking: 1 mark\n",
    "- All metrics calculated correctly: 1 mark\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b349fea",
   "metadata": {},
   "source": [
    "2.1 LSTM/GRU Architecture Design\n",
    "TODO: Choose LSTM or GRU\n",
    "TODO: Design architecture with stacked layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87daaa54",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_rnn_model(model_type, input_shape, hidden_units, n_layers, output_size):\n",
    "    \"\"\"\n",
    "    Build LSTM or GRU model\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    \n",
    "    # Stack layers\n",
    "    for i in range(n_layers):\n",
    "        return_seq = (i < n_layers - 1)\n",
    "        if model_type == 'LSTM':\n",
    "            model.add(layers.LSTM(hidden_units, return_sequences=return_seq))\n",
    "        elif model_type == 'GRU':\n",
    "            model.add(layers.GRU(hidden_units, return_sequences=return_seq))\n",
    "            \n",
    "    model.add(layers.Dense(output_size))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9e46a1",
   "metadata": {},
   "source": [
    "# Create RNN model\n",
    "rnn_model = build_rnn_model('LSTM', (sequence_length, n_features), 64, 2, prediction_horizon)\n",
    "rnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bf8c67",
   "metadata": {},
   "source": [
    "TODO: Compile model\n",
    "For Keras: model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "For PyTorch: define optimizer and loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44df1c4b",
   "metadata": {},
   "source": [
    "2.2 Train RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebf1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RNN MODEL TRAINING\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e65f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track training time\n",
    "rnn_start_time = time.time()\n",
    "\n",
    "history_rnn = rnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,  # 20 epochs is enough for demo\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d32150",
   "metadata": {},
   "source": [
    "TODO: Train your model\n",
    "For Keras: history = rnn_model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "For PyTorch: write training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b093f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_training_time = time.time() - rnn_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ade3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED: Track initial and final loss\n",
    "rnn_initial_loss = history_rnn.history['loss'][0]\n",
    "rnn_final_loss = history_rnn.history['loss'][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49554c49",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(f\"Training completed in {rnn_training_time:.2f} seconds\")\n",
    "print(f\"Initial Loss: {rnn_initial_loss:.4f}\")\n",
    "print(f\"Final Loss: {rnn_final_loss:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792870c6",
   "metadata": {},
   "source": [
    "2.3 Evaluate RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f459c549",
   "metadata": {},
   "source": [
    "TODO: Make predictions on test set\n",
    "TODO: Inverse transform if data was normalized\n",
    "TODO: Calculate all 4 required metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1678f898",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_mape(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Percentage Error\"\"\"\n",
    "    # TODO: Implement MAPE calculation\n",
    "    # MAPE = mean(|y_true - y_pred| / |y_true|) * 100\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dcb302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED: Calculate all 4 metrics\n",
    "y_pred_rnn = rnn_model.predict(X_test)\n",
    "\n",
    "rnn_mae = mean_absolute_error(y_test, y_pred_rnn)\n",
    "rnn_rmse = np.sqrt(mean_squared_error(y_test, y_pred_rnn))\n",
    "rnn_mape = np.mean(np.abs((y_test - y_pred_rnn) / y_test)) * 100\n",
    "rnn_r2 = r2_score(y_test, y_pred_rnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb4ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRNN Model Performance:\")\n",
    "print(f\"MAE:   {rnn_mae:.4f}\")\n",
    "print(f\"RMSE:  {rnn_rmse:.4f}\")\n",
    "print(f\"MAPE:  {rnn_mape:.4f}%\")\n",
    "print(f\"R\u00b2 Score: {rnn_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5be5a3",
   "metadata": {},
   "source": [
    "2.4 Visualize RNN Results\n",
    "TODO: Plot training loss curve\n",
    "TODO: Plot actual vs predicted values\n",
    "TODO: Plot residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d84b3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "PART 3: TRANSFORMER IMPLEMENTATION (5 MARKS)\n",
    "================================================================================\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Build Transformer encoder using Keras/PyTorch layers\n",
    "- MUST add positional encoding to input:\n",
    "  * Custom sinusoidal implementation OR\n",
    "  * Use built-in positional encoding (if framework provides)\n",
    "- Use torch.nn.TransformerEncoder or keras.layers.MultiHeadAttention\n",
    "- Use standard training methods\n",
    "- Track initial_loss and final_loss\n",
    "\n",
    "PROHIBITED:\n",
    "- Using pre-trained transformers (HuggingFace, TimeGPT, etc.)\n",
    "- Skipping positional encoding entirely\n",
    "\n",
    "GRADING:\n",
    "- Positional encoding added: 1 mark\n",
    "- Transformer architecture properly configured: 2 marks\n",
    "- Training completed with loss tracking: 1 mark\n",
    "- All metrics calculated correctly: 1 mark\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044873c0",
   "metadata": {},
   "source": [
    "3.1 Positional Encoding Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f5f6b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def positional_encoding(seq_length, d_model):\n",
    "    \"\"\"\n",
    "    Generate sinusoidal positional encodings\n",
    "    \"\"\"\n",
    "    position = np.arange(seq_length)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    \n",
    "    pe = np.zeros((seq_length, d_model))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    \n",
    "    return tf.cast(pe, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83741f21",
   "metadata": {},
   "source": [
    "3.2 Transformer Encoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1c4463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Using PyTorch\n",
    "\"\"\"\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_features, d_model, n_heads, n_layers, d_ff, output_size):\n",
    "        super().__init__()\n",
    "        self.input_projection = nn.Linear(n_features, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)  # Add positional encoding\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.fc = nn.Linear(d_model, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        x = self.pos_encoder(x)  # Add positional encoding\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        return self.fc(x)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33435a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Using Keras\n",
    "def build_transformer_model(seq_length, n_features, d_model, n_heads, n_layers, d_ff, output_size):\n",
    "    inputs = layers.Input(shape=(seq_length, n_features))\n",
    "    \n",
    "    # Project to d_model\n",
    "    x = layers.Dense(d_model)(inputs)\n",
    "    \n",
    "    # Add positional encoding\n",
    "    pe = positional_encoding(seq_length, d_model)\n",
    "    x = x + pe\n",
    "    \n",
    "    # Stack transformer encoder layers\n",
    "    for _ in range(n_layers):\n",
    "        # Multi-head attention\n",
    "        attn_output = layers.MultiHeadAttention(\n",
    "            num_heads=n_heads, \n",
    "            key_dim=d_model // n_heads\n",
    "        )(x, x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "        \n",
    "        # Feed-forward\n",
    "        ffn_output = layers.Dense(d_ff, activation='relu')(x)\n",
    "        ffn_output = layers.Dense(d_model)(ffn_output)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "    \n",
    "    # Output\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(output_size)(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mse'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f72750",
   "metadata": {},
   "source": [
    "3.3 Build Your Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Transformer Model\n",
    "transformer_model = build_transformer_model(\n",
    "    sequence_length, n_features, \n",
    "    d_model=64, n_heads=4, n_layers=2, d_ff=128, \n",
    "    output_size=prediction_horizon\n",
    ")\n",
    "transformer_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db4f26b",
   "metadata": {},
   "source": [
    "TODO: Create Transformer model using PyTorch or Keras\n",
    "Example for PyTorch:\n",
    "transformer_model = TransformerModel(n_features, d_model=64, n_heads=4, n_layers=2, d_ff=256, output_size=prediction_horizon)\n",
    "Example for Keras:\n",
    "transformer_model = build_transformer_model(sequence_length, n_features, d_model=64, n_heads=4, n_layers=2, d_ff=256, output_size=prediction_horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1bc8c9",
   "metadata": {},
   "source": [
    "TODO: Define optimizer and loss\n",
    "For PyTorch: optimizer = torch.optim.Adam(transformer_model.parameters(), lr=0.001); criterion = nn.MSELoss()\n",
    "For Keras: model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "For PyTorch: define optimizer and loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00d6c9c",
   "metadata": {},
   "source": [
    "3.4 Train Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2325454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRANSFORMER MODEL TRAINING\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed1263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track training time\n",
    "transformer_start_time = time.time()\n",
    "\n",
    "history_transformer = transformer_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846c1aab",
   "metadata": {},
   "source": [
    "TODO: Train your model\n",
    "For Keras: history = transformer_model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
    "For PyTorch: write training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_training_time = time.time() - transformer_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9e89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED: Track initial and final loss\n",
    "transformer_initial_loss = history_transformer.history['loss'][0]\n",
    "transformer_final_loss = history_transformer.history['loss'][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc7240",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training completed in {transformer_training_time:.2f} seconds\")\n",
    "print(f\"Initial Loss: {transformer_initial_loss:.4f}\")\n",
    "print(f\"Final Loss: {transformer_final_loss:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3707b1e",
   "metadata": {},
   "source": [
    "3.5 Evaluate Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6e3fa1",
   "metadata": {},
   "source": [
    "TODO: Make predictions on test set\n",
    "TODO: Inverse transform if data was normalized\n",
    "TODO: Calculate all 4 required metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e024b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED: Calculate all 4 metrics\n",
    "y_pred_trans = transformer_model.predict(X_test)\n",
    "\n",
    "transformer_mae = mean_absolute_error(y_test, y_pred_trans)\n",
    "transformer_rmse = np.sqrt(mean_squared_error(y_test, y_pred_trans))\n",
    "transformer_mape = np.mean(np.abs((y_test - y_pred_trans) / y_test)) * 100\n",
    "transformer_r2 = r2_score(y_test, y_pred_trans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0126e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTransformer Model Performance:\")\n",
    "print(f\"MAE:   {transformer_mae:.4f}\")\n",
    "print(f\"RMSE:  {transformer_rmse:.4f}\")\n",
    "print(f\"MAPE:  {transformer_mape:.4f}%\")\n",
    "print(f\"R\u00b2 Score: {transformer_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d3a7f0",
   "metadata": {},
   "source": [
    "3.6 Visualize Transformer Results\n",
    "TODO: Plot training loss curve\n",
    "TODO: Plot actual vs predicted values\n",
    "TODO: Plot attention weights (optional but informative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98256c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "PART 4: MODEL COMPARISON AND VISUALIZATION (Informational)\n",
    "================================================================================\n",
    "\n",
    "Compare both models on:\n",
    "- Performance metrics\n",
    "- Training time\n",
    "- Model complexity\n",
    "- Convergence behavior\n",
    "- Ability to capture long-term dependencies\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e98c47",
   "metadata": {},
   "source": [
    "4.1 Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1fbf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0e5a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['MAE', 'RMSE', 'MAPE (%)', 'R\u00b2 Score', 'Training Time (s)', 'Parameters'],\n",
    "    'RNN (LSTM/GRU)': [\n",
    "        rnn_mae,\n",
    "        rnn_rmse,\n",
    "        rnn_mape,\n",
    "        rnn_r2,\n",
    "        rnn_training_time,\n",
    "        0  # TODO: Fill with RNN total parameters\n",
    "    ],\n",
    "    'Transformer': [\n",
    "        transformer_mae,\n",
    "        transformer_rmse,\n",
    "        transformer_mape,\n",
    "        transformer_r2,\n",
    "        transformer_training_time,\n",
    "        0  # TODO: Fill with Transformer total parameters\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a860057",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ac551d",
   "metadata": {},
   "source": [
    "4.2 Visual Comparison\n",
    "TODO: Create bar plot comparing metrics\n",
    "TODO: Plot predictions comparison (both models vs actual)\n",
    "TODO: Plot training curves comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22896627",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "PART 5: ANALYSIS (2 MARKS)\n",
    "================================================================================\n",
    "\n",
    "REQUIRED:\n",
    "- Write MAXIMUM 200 words (guideline - no marks deduction if exceeded)\n",
    "- Address key topics with depth\n",
    "\n",
    "GRADING (Quality-based):\n",
    "- Covers 5+ key topics with deep understanding: 2 marks\n",
    "- Covers 3-4 key topics with good understanding: 1 mark\n",
    "- Covers <3 key topics or superficial: 0 marks\n",
    "\n",
    "Key Topics:\n",
    "1. Performance comparison with specific metrics\n",
    "2. RNN vs Transformer architecture advantages\n",
    "3. Impact of attention mechanism vs recurrent connections\n",
    "4. Long-term dependency handling comparison\n",
    "5. Computational cost comparison\n",
    "6. Convergence behavior differences\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05be677",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_text = \"\"\"\n",
    "1. Performance Comparison: The RNN (LSTM) and Transformer models were compared on IBM stock data. \n",
    "   The LSTM typically shows strong performance on sequential patterns, but Transformers can capture long-range dependencies efficiently.\n",
    "   (Note: Actual metrics will depend on the run, but we expect competitive performance).\n",
    "\n",
    "2. RNN vs Transformer Advantages: RNNs process data sequentially, making them naturally suited for time-series but slower to train on long sequences.\n",
    "   Transformers process data in parallel using self-attention, allowing for faster training and better handling of long-term dependencies.\n",
    "\n",
    "3. Impact of Attention: The attention mechanism allows the model to focus on relevant past time steps directly, reducing the vanishing gradient problem.\n",
    "\n",
    "4. Long-term Dependencies: Transformers excel here as the path length between any two positions is constant O(1), whereas in RNNs it is O(n).\n",
    "\n",
    "5. Computational Cost: Transformers have higher memory requirements (O(n^2) attention map) but train faster due to parallelism. RNNs are more memory efficient but sequential.\n",
    "\n",
    "6. Convergence: Transformers often converge faster in terms of epochs but may require more data to generalize compared to LSTMs.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05df3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED: Print analysis with word count\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(analysis_text)\n",
    "print(\"=\"*70)\n",
    "print(f\"Analysis word count: {len(analysis_text.split())} words\")\n",
    "if len(analysis_text.split()) > 200:\n",
    "    print(\"\u26a0\ufe0f  Warning: Analysis exceeds 200 words (guideline)\")\n",
    "else:\n",
    "    print(\"\u2713 Analysis within word count guideline\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f2ce90",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "PART 6: ASSIGNMENT RESULTS SUMMARY (REQUIRED FOR AUTO-GRADING)\n",
    "================================================================================\n",
    "\n",
    "DO NOT MODIFY THE STRUCTURE BELOW\n",
    "This JSON output is used by the auto-grader\n",
    "Ensure all field names are EXACT\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c800bf2e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_assignment_results():\n",
    "    \"\"\"\n",
    "    Generate complete assignment results in required format\n",
    "    \n",
    "    Returns:\n",
    "        dict: Complete results with all required fields\n",
    "    \"\"\"\n",
    "    \n",
    "    framework_used = \"keras\"  # TODO: Change to \"pytorch\" if using PyTorch\n",
    "    rnn_model_type = \"LSTM\"  # TODO: Change to \"GRU\" if using GRU\n",
    "    \n",
    "    results = {\n",
    "        # Dataset Information\n",
    "        'dataset_name': dataset_name,\n",
    "        'dataset_source': dataset_source,\n",
    "        'n_samples': n_samples,\n",
    "        'n_features': n_features,\n",
    "        'sequence_length': sequence_length,\n",
    "        'prediction_horizon': prediction_horizon,\n",
    "        'problem_type': problem_type,\n",
    "        'primary_metric': primary_metric,\n",
    "        'metric_justification': metric_justification,\n",
    "        'train_samples': train_samples,\n",
    "        'test_samples': test_samples,\n",
    "        'train_test_ratio': train_test_ratio,\n",
    "        \n",
    "        # RNN Model Results\n",
    "        'rnn_model': {\n",
    "            'framework': framework_used,\n",
    "            'model_type': rnn_model_type,\n",
    "            'architecture': {\n",
    "                'n_layers': 2,  # TODO: Number of stacked layers\n",
    "                'hidden_units': 64,  # TODO: Hidden units per layer\n",
    "                'total_parameters': rnn_model.count_params()  # TODO: Calculate total parameters\n",
    "            },\n",
    "            'training_config': {\n",
    "                'learning_rate': 0.001,  # TODO: Your actual learning rate\n",
    "                'n_epochs': 20,  # TODO: Your actual epochs\n",
    "                'batch_size': 32,  # TODO: Your actual batch size\n",
    "                'optimizer': 'Adam',  # TODO: Your actual optimizer\n",
    "                'loss_function': 'MSE'  # TODO: Your actual loss\n",
    "            },\n",
    "            'initial_loss': rnn_initial_loss,\n",
    "            'final_loss': rnn_final_loss,\n",
    "            'training_time_seconds': rnn_training_time,\n",
    "            'mae': rnn_mae,\n",
    "            'rmse': rnn_rmse,\n",
    "            'mape': rnn_mape,\n",
    "            'r2_score': rnn_r2\n",
    "        },\n",
    "        \n",
    "        # Transformer Model Results\n",
    "        'transformer_model': {\n",
    "            'framework': framework_used,\n",
    "            'architecture': {\n",
    "                'n_layers': 2,  # TODO: Number of transformer layers\n",
    "                'n_heads': 4,  # TODO: Number of attention heads\n",
    "                'd_model': 64,  # TODO: Model dimension\n",
    "                'd_ff': 128,  # TODO: Feed-forward dimension\n",
    "                'has_positional_encoding': True,  # MUST be True\n",
    "                'has_attention': True,  # MUST be True\n",
    "                'total_parameters': rnn_model.count_params()  # TODO: Calculate total parameters\n",
    "            },\n",
    "            'training_config': {\n",
    "                'learning_rate': 0.001,  # TODO: Your actual learning rate\n",
    "                'n_epochs': 20,  # TODO: Your actual epochs\n",
    "                'batch_size': 32,  # TODO: Your actual batch size\n",
    "                'optimizer': 'Adam',  # TODO: Your actual optimizer\n",
    "                'loss_function': 'MSE'  # TODO: Your actual loss\n",
    "            },\n",
    "            'initial_loss': transformer_initial_loss,\n",
    "            'final_loss': transformer_final_loss,\n",
    "            'training_time_seconds': transformer_training_time,\n",
    "            'mae': transformer_mae,\n",
    "            'rmse': transformer_rmse,\n",
    "            'mape': transformer_mape,\n",
    "            'r2_score': transformer_r2\n",
    "        },\n",
    "        \n",
    "        # Analysis\n",
    "        'analysis': analysis_text,\n",
    "        'analysis_word_count': len(analysis_text.split()),\n",
    "        \n",
    "        # Training Success Indicators\n",
    "        'rnn_loss_decreased': rnn_final_loss < rnn_initial_loss if rnn_initial_loss and rnn_final_loss else False,\n",
    "        'transformer_loss_decreased': transformer_final_loss < transformer_initial_loss if transformer_initial_loss and transformer_final_loss else False,\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694a274d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and print results\n",
    "try:\n",
    "    assignment_results = get_assignment_results()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ASSIGNMENT RESULTS SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(json.dumps(assignment_results, indent=2))\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce921018",
   "metadata": {},
   "outputs": [],
   "source": [
    "except Exception as e:\n",
    "    print(f\"\\n\u26a0\ufe0f  ERROR generating results: {str(e)}\")\n",
    "    print(\"Please ensure all variables are properly defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f84f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "FINAL CHECKLIST - VERIFY BEFORE SUBMISSION\n",
    "================================================================================\n",
    "\n",
    "\u25a1 Student information filled at the top (BITS ID, Name, Email)\n",
    "\u25a1 Filename is <BITS_ID>_rnn_assignment.ipynb\n",
    "\u25a1 All cells executed (Kernel \u2192 Restart & Run All)\n",
    "\u25a1 All outputs visible\n",
    "\u25a1 LSTM/GRU implemented with stacked layers\n",
    "\u25a1 Positional encoding implemented (sinusoidal)\n",
    "\u25a1 Multi-head attention implemented (Q, K, V, scaled dot-product)\n",
    "\u25a1 Both models use Keras or PyTorch\n",
    "\u25a1 Both models trained with loss tracking (initial_loss and final_loss)\n",
    "\u25a1 All 4 metrics calculated for both models (MAE, RMSE, MAPE, R\u00b2)\n",
    "\u25a1 Temporal train/test split used (NO shuffling)\n",
    "\u25a1 Primary metric selected and justified\n",
    "\u25a1 Analysis written (quality matters, not just word count)\n",
    "\u25a1 Visualizations created\n",
    "\u25a1 Assignment results JSON printed at the end\n",
    "\u25a1 No execution errors in any cell\n",
    "\u25a1 File opens without corruption\n",
    "\u25a1 Submit ONLY .ipynb file (NO zip, NO data files, NO images)\n",
    "\u25a1 Screenshot of environment with account details included\n",
    "\u25a1 Only one submission attempt\n",
    "\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6d14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "================================================================================\n",
    "ENVIRONMENT VERIFICATION - SCREENSHOT REQUIRED\n",
    "================================================================================\n",
    "\n",
    "IMPORTANT: Take a screenshot of your environment showing account details\n",
    "\n",
    "For Google Colab:\n",
    "- Click on your profile icon (top right)\n",
    "- Screenshot should show your email/account clearly\n",
    "- Include the entire Colab interface with notebook name visible\n",
    "\n",
    "For BITS Virtual Lab:\n",
    "- Screenshot showing your login credentials/account details\n",
    "- Include the entire interface with your username/session info visible\n",
    "\n",
    "Paste the screenshot below this cell or in a new markdown cell.\n",
    "This helps verify the work was done by you in your environment.\n",
    "\n",
    "================================================================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e01e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display system information\n",
    "import platform\n",
    "import sys\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a2fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ENVIRONMENT INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n\u26a0\ufe0f  REQUIRED: Add screenshot of your Google Colab/BITS Virtual Lab\")\n",
    "print(\"showing your account details in the cell below this one.\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}